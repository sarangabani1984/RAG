{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "1287de69-10a3-45b4-9045-f2d19d08124d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "1️⃣ Install Required Packages"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-docx in /local_disk0/.ephemeral_nfs/envs/pythonEnv-cb72e526-b12e-4874-959e-b2a848001b63/lib/python3.12/site-packages (1.2.0)\nRequirement already satisfied: PyPDF2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-cb72e526-b12e-4874-959e-b2a848001b63/lib/python3.12/site-packages (3.0.1)\nRequirement already satisfied: langchain-text-splitters in /local_disk0/.ephemeral_nfs/envs/pythonEnv-cb72e526-b12e-4874-959e-b2a848001b63/lib/python3.12/site-packages (1.0.0)\nRequirement already satisfied: lxml>=3.1.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-cb72e526-b12e-4874-959e-b2a848001b63/lib/python3.12/site-packages (from python-docx) (6.0.2)\nRequirement already satisfied: typing_extensions>=4.9.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-cb72e526-b12e-4874-959e-b2a848001b63/lib/python3.12/site-packages (from python-docx) (4.15.0)\nRequirement already satisfied: langchain-core<2.0.0,>=1.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-cb72e526-b12e-4874-959e-b2a848001b63/lib/python3.12/site-packages (from langchain-text-splitters) (1.1.3)\nRequirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-cb72e526-b12e-4874-959e-b2a848001b63/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (1.33)\nRequirement already satisfied: langsmith<1.0.0,>=0.3.45 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-cb72e526-b12e-4874-959e-b2a848001b63/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.4.56)\nRequirement already satisfied: packaging<26.0.0,>=23.2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-cb72e526-b12e-4874-959e-b2a848001b63/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (25.0)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-cb72e526-b12e-4874-959e-b2a848001b63/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (2.12.5)\nRequirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-cb72e526-b12e-4874-959e-b2a848001b63/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (6.0.3)\nRequirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /databricks/python3/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (9.0.0)\nRequirement already satisfied: uuid-utils<1.0,>=0.12.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-cb72e526-b12e-4874-959e-b2a848001b63/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.12.0)\nRequirement already satisfied: jsonpointer>=1.9 in /databricks/python3/lib/python3.12/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (3.0.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /databricks/python3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.27.0)\nRequirement already satisfied: orjson>=3.9.14 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-cb72e526-b12e-4874-959e-b2a848001b63/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (3.11.5)\nRequirement already satisfied: requests-toolbelt>=1.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-cb72e526-b12e-4874-959e-b2a848001b63/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (1.0.0)\nRequirement already satisfied: requests>=2.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-cb72e526-b12e-4874-959e-b2a848001b63/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (2.32.5)\nRequirement already satisfied: zstandard>=0.23.0 in /databricks/python3/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.23.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-cb72e526-b12e-4874-959e-b2a848001b63/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-cb72e526-b12e-4874-959e-b2a848001b63/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (2.41.5)\nRequirement already satisfied: typing-inspection>=0.4.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-cb72e526-b12e-4874-959e-b2a848001b63/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.4.2)\nRequirement already satisfied: anyio in /local_disk0/.ephemeral_nfs/envs/pythonEnv-cb72e526-b12e-4874-959e-b2a848001b63/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (4.12.0)\nRequirement already satisfied: certifi in /local_disk0/.ephemeral_nfs/envs/pythonEnv-cb72e526-b12e-4874-959e-b2a848001b63/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (2025.11.12)\nRequirement already satisfied: httpcore==1.* in /local_disk0/.ephemeral_nfs/envs/pythonEnv-cb72e526-b12e-4874-959e-b2a848001b63/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (1.0.9)\nRequirement already satisfied: idna in /local_disk0/.ephemeral_nfs/envs/pythonEnv-cb72e526-b12e-4874-959e-b2a848001b63/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (3.11)\nRequirement already satisfied: sniffio in /databricks/python3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (1.3.0)\nRequirement already satisfied: h11>=0.16 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-cb72e526-b12e-4874-959e-b2a848001b63/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.16.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-cb72e526-b12e-4874-959e-b2a848001b63/lib/python3.12/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (3.4.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-cb72e526-b12e-4874-959e-b2a848001b63/lib/python3.12/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (2.6.1)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install python-docx PyPDF2 langchain-text-splitters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2076fcdb-bcba-44bf-9e57-150890150611",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<bound method DBUtils.LibraryHandler.restartPython of Package 'dbutils.library'.>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.library.restartPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e021097-5f14-4dd7-9c99-bc9f4c295e48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDD0D Scanning directory: /Volumes/sco_dev/sarang84/pdc_app\n\n\uD83D\uDCC4 Found file: J2.05.P05.M02_Change Management Approver and Observer Matrix_R41 (1).docx (docx)\n✅ Loaded: /Volumes/sco_dev/sarang84/pdc_app/J2.05.P05.M02_Change Management Approver and Observer Matrix_R41 (1).docx\n   Preview:    J2.05.P05.M02  Global Change Management Approver and Observer Matrix               What is the goal of this work instruction.  Be as descriptive as possible.101.1. PURPOSE: The purpose of this docu ...\n\n\n\uD83D\uDCC4 Found file: J2.05.P05.W01 ECO_WI_R52.doc (doc)\n⏭️ Skipping unsupported file: J2.05.P05.W01 ECO_WI_R52.doc\n\n\uD83D\uDCC4 Found file: J2.05.P07.W01_MCO_WI_R63.docx (docx)\n✅ Loaded: /Volumes/sco_dev/sarang84/pdc_app/J2.05.P07.W01_MCO_WI_R63.docx\n   Preview:        J2.05.P07.W01,  Business Process, MCO Work Instructions     PURPOSE:  The purpose of this document is to describe the process necessary to create, submit, route, review, approve and release a M ...\n\n\n\uD83D\uDCC4 Found file: J2.05.P07_R20__MCO.pdf (pdf)\n✅ Loaded: /Volumes/sco_dev/sarang84/pdc_app/J2.05.P07_R20__MCO.pdf\n   Preview: J2.05.P01.T01_R05 - Proprietary and Confidential -- printed copies are for reference onlyRef: J2.05.P07 - Rev 20 Owner: Senthil Nathan K © Copyright 2003 -2025 Juniper Networks Inc. All rights reserve ...\n\n\n\uD83D\uDCC4 Found file: J4.03.P06.W01_Purge WI_R15.docx (docx)\n✅ Loaded: /Volumes/sco_dev/sarang84/pdc_app/J4.03.P06.W01_Purge WI_R15.docx\n   Preview:     J4.03.P06.W01 Business Process, Purge Work Instructions     PURPOSE: This work instruction describes the procedure for the Creating and Routing a Purge within AGILE.  AREAS AFFECTED: NA  TOOLS / M ...\n\n\n\uD83D\uDCC4 Found file: J4.03.P10.W01 MFG Hold WI_R06 (1).docx (docx)\n✅ Loaded: /Volumes/sco_dev/sarang84/pdc_app/J4.03.P10.W01 MFG Hold WI_R06 (1).docx\n   Preview:  J4.03.P10.W01 Business Process, Work Instructions, Manufacturing Hold   What is the goal of this work instruction.  Be as descriptive as possible.PURPOSE: The purpose of this document is to provide a ...\n\n\n\uD83D\uDCC4 Found file: PDC One Stop.docx (docx)\n✅ Loaded: /Volumes/sco_dev/sarang84/pdc_app/PDC One Stop.docx\n   Preview:         PDC ONE STOP DOCUMENT                Table of Contents 1.\tBusiness Scope\t5 2.\tNavigation Help\t5 2.1.\tExample using Windows 2003:\t5 2.2.\tFrom the Table of Contents (TOC) to the related section  ...\n\n\n\uD83D\uDCC4 Found file: PROC-10155_Rev_01 (2).docx (docx)\n✅ Loaded: /Volumes/sco_dev/sarang84/pdc_app/PROC-10155_Rev_01 (2).docx\n   Preview:        PURPOSE:   The purpose of this document is to outline the procedure for managing and executing the varies                  Part data cleanup processes by PDC team.  AREAS AFFECTED:   PDC CE  TO ...\n\n\n==============================\n\uD83D\uDCE6 DOCUMENT LOADING SUMMARY\n==============================\nTotal files loaded: 7\n1. /Volumes/sco_dev/sarang84/pdc_app/J2.05.P05.M02_Change Management Approver and Observer Matrix_R41 (1).docx (length=13983 chars)\n2. /Volumes/sco_dev/sarang84/pdc_app/J2.05.P07.W01_MCO_WI_R63.docx (length=23100 chars)\n3. /Volumes/sco_dev/sarang84/pdc_app/J2.05.P07_R20__MCO.pdf (length=7805 chars)\n4. /Volumes/sco_dev/sarang84/pdc_app/J4.03.P06.W01_Purge WI_R15.docx (length=14407 chars)\n5. /Volumes/sco_dev/sarang84/pdc_app/J4.03.P10.W01 MFG Hold WI_R06 (1).docx (length=11515 chars)\n6. /Volumes/sco_dev/sarang84/pdc_app/PDC One Stop.docx (length=78075 chars)\n7. /Volumes/sco_dev/sarang84/pdc_app/PROC-10155_Rev_01 (2).docx (length=10991 chars)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "from docx import Document as DocxDocument\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Location of your documents in Databricks Volume\n",
    "VOLUME_PATH = \"/Volumes/sco_dev/sarang84/pdc_app\"\n",
    "\n",
    "# Output Delta table\n",
    "SCHEMA = \"sco_dev\"\n",
    "USER = \"sarang84\"\n",
    "TABLE = \"pdc_RAG_Demo\"\n",
    "FULL_TABLE_NAME = f\"{SCHEMA}.{USER}.{TABLE}\"\n",
    "\n",
    "# Chunk Config\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# READER FUNCTIONS\n",
    "# ------------------------------\n",
    "def read_txt(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def read_docx(path):\n",
    "    doc = DocxDocument(path)\n",
    "    return \"\\n\".join([p.text for p in doc.paragraphs])\n",
    "\n",
    "def read_pdf(path):\n",
    "    reader = PdfReader(path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text() or \"\"\n",
    "    return text\n",
    "\n",
    "# ------------------------------\n",
    "# LOAD DOCUMENTS\n",
    "# ------------------------------\n",
    "all_docs = []\n",
    "\n",
    "base_path = VOLUME_PATH  \n",
    "\n",
    "print(\"\uD83D\uDD0D Scanning directory:\", base_path)\n",
    "\n",
    "for root, dirs, files in os.walk(base_path):\n",
    "    for file in files:\n",
    "        full_path = os.path.join(root, file)\n",
    "        ext = file.lower().split(\".\")[-1]\n",
    "\n",
    "        print(f\"\\n\uD83D\uDCC4 Found file: {file} ({ext})\")\n",
    "\n",
    "        try:\n",
    "            if ext == \"txt\":\n",
    "                text = read_txt(full_path)\n",
    "            elif ext == \"docx\":\n",
    "                text = read_docx(full_path)\n",
    "            elif ext == \"pdf\":\n",
    "                text = read_pdf(full_path)\n",
    "            else:\n",
    "                print(f\"⏭️ Skipping unsupported file: {file}\")\n",
    "                continue\n",
    "\n",
    "            # Store document entry\n",
    "            cleaned_path = full_path.replace(\"/dbfs\", \"\")\n",
    "            all_docs.append({\n",
    "                \"source\": cleaned_path,\n",
    "                \"text\": text\n",
    "            })\n",
    "\n",
    "            # Print preview\n",
    "            print(f\"✅ Loaded: {cleaned_path}\")\n",
    "            print(\"   Preview:\", text[:200].replace(\"\\n\", \" \"), \"...\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error reading {file}: {e}\")\n",
    "\n",
    "# ------------------------------\n",
    "# SUMMARY\n",
    "# ------------------------------\n",
    "print(\"\\n==============================\")\n",
    "print(\"\uD83D\uDCE6 DOCUMENT LOADING SUMMARY\")\n",
    "print(\"==============================\")\n",
    "print(f\"Total files loaded: {len(all_docs)}\")\n",
    "\n",
    "for i, doc in enumerate(all_docs):\n",
    "    print(f\"{i+1}. {doc['source']} (length={len(doc['text'])} chars)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03941f95-b3af-4032-9e5e-dfe19aeab2ab",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Split Loaded Documents into Chunks"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n==============================\n\uD83E\uDDE9 CHUNKING SUMMARY\n==============================\nTotal chunks generated: 215\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# CHUNK DOCUMENTS\n",
    "# ------------------------------\n",
    "all_chunks = []\n",
    "\n",
    "for doc in all_docs:\n",
    "    chunks = splitter.split_text(doc[\"text\"])\n",
    "\n",
    "    for chunk in chunks:\n",
    "        all_chunks.append({\n",
    "            \"source\": doc[\"source\"],\n",
    "            \"chunk\": chunk,\n",
    "            \"chunk_id\": hashlib.md5(chunk.encode()).hexdigest(),\n",
    "            \"ingested_at\": datetime.now()\n",
    "        })\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(\"\uD83E\uDDE9 CHUNKING SUMMARY\")\n",
    "print(\"==============================\")\n",
    "print(f\"Total chunks generated: {len(all_chunks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a593764-1ed0-4865-b800-0f2c33332521",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Convert chunks to Spark DataFrame"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------+--------------------------+-----------------------------------------------------------------------------------------------------------+\n|chunk                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |chunk_id                        |ingested_at               |source                                                                                                     |\n+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------+--------------------------+-----------------------------------------------------------------------------------------------------------+\n|J2.05.P05.M02\\n\\nGlobal Change Management Approver and Observer Matrix\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat is the goal of this work instruction.  Be as descriptive as possible.101.1. PURPOSE:\\nThe purpose of this document is to provide a consolidated single source reference guide Approval Matrix for Approver/Observer and CM Confirmation status encompassing all Change Types using the Agile Product Lifecycle Management (PLM) Application.\\n\\n2. SCOPE:\\nTransfer of all Approval/Observer/CM Confirmation requirements from various PDC Work Instructions for ease of reference and updates.\\n\\n3.  AFFECTED AREAS\\nProduct Data ControlShould include the users who needs to follow this work instruction.  The owner / team who is responsible for updating this work instruction.  Other teams who are engaged or affected by this work instruction.                                                                                                                           |9f37df5dd38e51e28ebfebeae64049d6|2025-12-11 11:18:51.259678|/Volumes/sco_dev/sarang84/pdc_app/J2.05.P05.M02_Change Management Approver and Observer Matrix_R41 (1).docx|\n|4. REFERENCE DOCUMENTS:\\nJ2.05.P05.W01\\tBusiness Process, ECO Work Instructions \\nJ2.05.P06.W01\\tBusiness Process, Deviation Work Instructions\\nJ2.05.P07.W01\\tBusiness Process, MCO Work Instructions - Global\\nJ4.03.P01.W04\\tBusiness Process, Field Change Order Work Instructions\\nJ4.03.P01.W01\\tBusiness Process, Manufacturing Hold Work Instructions\\nJ2.05.P05.W13\\tBusiness Process, Global Product Number Request / Maintenance\\nJ4.03.P06.W01\\tBusiness Process, Purge Work Instructions\\nJ4.03.P04.W01 \\tBusiness Process, Stop-Ship Work Instructions\\nJ2.05.P05.W17\\tBusiness Process, Global Work Instructions LHS - Limited Hardware Ship\\nJ3.03.P01.W02\\tBusiness Process, Work Instructions, Part Description Standards\\nJ2.05.P05.M01\\tECO Approvers/Observers Matrix\\nPROC-9977\\tManufacturer Part Number (MPN) Lifecycle Guideline Document\\n\\n\\n5. EXPLANATION of TERMS:                                                                                              |0d46f455a0efce8a6621fb9446720920|2025-12-11 11:18:51.259689|/Volumes/sco_dev/sarang84/pdc_app/J2.05.P05.M02_Change Management Approver and Observer Matrix_R41 (1).docx|\n|5. EXPLANATION of TERMS:\\n\\n\\n\\n\\n6. Engineering Change Order CCB Approvers/Observers:\\nECO CCB Approval/Observer distribution lists are pre-defined and assigned based off the associated  product Name and Product Lifecycle – Reference Agile Address Book, J2.05.P05.M01 and J2.05.P05.W01.\\n\\n\\n\\nEngineering Change Order CM Confirmation Approvers/Observers: \\n\\n\\n6.1 Manufacturing Test Documents:\\n\\nApprovers and Observers for Product Name specific document releases are defined below. \\n\\nPROCs that are non-product specific will continue follow the current process of seeking the list of Approver(s) and Observer(s) from the respective Originator.\\n\\n\\nNote: *Individual ACE please refer share point for the ACE approval matrix\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n6.2 Non-Design Change Affecting Multiple Products:                                                                                                                                            |fe486f55b865dd05525be9431de7ad89|2025-12-11 11:18:51.259697|/Volumes/sco_dev/sarang84/pdc_app/J2.05.P05.M02_Change Management Approver and Observer Matrix_R41 (1).docx|\n|If an ECR/ECO is submitted for common Items such as Labels, Packaging and Tech pubs etc. which affects multiple programs, adding respective product distribution list will have a big list of approvers. In such cases, the below steps to be followed:\\nShare the below table with the originator along with the list of affected products (NPI/Sustaining). \\nOriginator to review and identify the required approver role(s) from the given table depends on the type of change and provide the approvers list and mark N/A for any roles that are deemed identified as not required for the given specific change. \\nChange Analyst to insist the originator to provide one Approver Rep per Role/Function preferably.\\nAdd the provided list as approvers & expand the respective Product DL Approvers & Observers in the Observers section.\\nAttach the where used report in the ECO attachments.\\nAttach the originator email with custom approvers filled table in the ECO attachments|0221fbe17da48af9a62970c1c8794c48|2025-12-11 11:18:51.259703|/Volumes/sco_dev/sarang84/pdc_app/J2.05.P05.M02_Change Management Approver and Observer Matrix_R41 (1).docx|\n|Few examples of such cases are:\\n-\\t520 part - Label/ Serial Number, shipping label\\n-\\t530 part - Pub/License agreement\\n-\\t550 part – Packaging\\n- \\t420 part – Dust cover\\n\\n\\n\\n\\n\\n6.3 MFGTEST-LOOPBACK/ MFG TEST TOOL\\nIf an ECR/ECO, MCO, DEV is submitted for the parts with Product Name as MFGTEST-LOOPBACK/ MFG TEST TOOL, below Distribution List to be followed:\\n\\n \\n\\n6.4 Product Number CCB Approvals/Observers\\nNote: Prototype/Forecast, Pilot/PP, Production/Ship approvers and observers are defined on J2.05.P05.M01\\n\\n6.4.1 Hardware Product Numbers: \\n\\n* All required Approvers/Observers for the PLANNING User Item Type will be auto populated.\\n\\n** If New Product Number request is to support Sustaining Product add respective CM Manager as approver in place of MPM and NPI Materials and exclude EPM in the observer list.\\n\\n*** If the New Product Number is for sustaining product, CSS-NPI PM can be excluded.\\n\\n6.4.2 License Product Numbers:     |10a4a69d85744046ad6e62f3b298761f|2025-12-11 11:18:51.259709|/Volumes/sco_dev/sarang84/pdc_app/J2.05.P05.M02_Change Management Approver and Observer Matrix_R41 (1).docx|\n+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------+--------------------------+-----------------------------------------------------------------------------------------------------------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df_chunks = spark.createDataFrame(all_chunks)\n",
    "df_chunks.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78ffd1c0-84f3-4b3c-9f71-c040e6a54d1f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Save Chunk Data into a Delta Table"
    }
   },
   "outputs": [],
   "source": [
    "df_chunks.write.mode(\"overwrite\").format(\"delta\").saveAsTable(FULL_TABLE_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8b01c81-4443-4580-b274-762ca4784173",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n|COUNT(*)|\n+--------+\n|     215|\n+--------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"SELECT COUNT(*) FROM {FULL_TABLE_NAME}\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e216411d-584c-40db-8baf-ef1f7143eef2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: databricks-sdk in /local_disk0/.ephemeral_nfs/envs/pythonEnv-cb72e526-b12e-4874-959e-b2a848001b63/lib/python3.12/site-packages (0.74.0)\nRequirement already satisfied: requests<3,>=2.28.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-cb72e526-b12e-4874-959e-b2a848001b63/lib/python3.12/site-packages (from databricks-sdk) (2.32.5)\nRequirement already satisfied: google-auth~=2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-cb72e526-b12e-4874-959e-b2a848001b63/lib/python3.12/site-packages (from databricks-sdk) (2.43.0)\nRequirement already satisfied: protobuf!=5.26.*,!=5.27.*,!=5.28.*,!=5.29.0,!=5.29.1,!=5.29.2,!=5.29.3,!=5.29.4,!=6.30.0,!=6.30.1,!=6.31.0,<7.0,>=4.25.8 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-cb72e526-b12e-4874-959e-b2a848001b63/lib/python3.12/site-packages (from databricks-sdk) (6.33.2)\nRequirement already satisfied: cachetools<7.0,>=2.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-cb72e526-b12e-4874-959e-b2a848001b63/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk) (6.2.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-cb72e526-b12e-4874-959e-b2a848001b63/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk) (0.4.2)\nRequirement already satisfied: rsa<5,>=3.1.4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-cb72e526-b12e-4874-959e-b2a848001b63/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk) (4.9.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-cb72e526-b12e-4874-959e-b2a848001b63/lib/python3.12/site-packages (from requests<3,>=2.28.1->databricks-sdk) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-cb72e526-b12e-4874-959e-b2a848001b63/lib/python3.12/site-packages (from requests<3,>=2.28.1->databricks-sdk) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-cb72e526-b12e-4874-959e-b2a848001b63/lib/python3.12/site-packages (from requests<3,>=2.28.1->databricks-sdk) (2.6.1)\nRequirement already satisfied: certifi>=2017.4.17 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-cb72e526-b12e-4874-959e-b2a848001b63/lib/python3.12/site-packages (from requests<3,>=2.28.1->databricks-sdk) (2025.11.12)\nRequirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-cb72e526-b12e-4874-959e-b2a848001b63/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk) (0.6.1)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "pip install databricks-sdk --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee8ec2e8-5cc4-44f0-bad1-ab68084a5158",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Demo_RAG_PDC",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}